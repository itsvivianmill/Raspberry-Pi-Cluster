<h2>1.0 Raspberry Pi 400 Cluster</h2>

<h4>Materials:</h4>

- 1 x Switch
- 3 x Raspberry Pi 400 Series

<h4>References: Building a Raspberry Cluster</h4>
1. https://www.raspberrypi.com/tutorials/cluster-raspberry-pi-tutorial/ <br>
2. https://projects.raspberrypi.org/en/projects/build-an-octapi <br>
3. https://forums.raspberrypi.com/viewtopic.php?t=328584 <br>
4. https://deninet.com/creation/2022/01/30/components-and-rack-pi-cluster <br>
5. https://www.youtube.com/watch?v=H2rTecSO0gk <br>
<br>

<h3>1.1: Editing SSH Files</h3>

We started by following the [NetworkChuck Raspberry Pi](https://www.youtube.com/watch?v=X9fSMGkjtug) video and set up our Raspberry Pi with the Raspberry Pi Lite 32-bit OS (later changed to 64-bit).

The device we used to access and configure the micro SSD in the Pi is linked [here](https://www.amazon.com/gp/aw/d/B081VHSB2V/?_encoding=UTF8&pd_rd_plhdr=t&aaxitk=5b290797ca3079a2b9552be521291b69&hsa_cr_id=6748414440801&qid=1704734333&sr=1-1-9e67e56a-6f64-441f-a281-df67fc737124&ref_=sbx_be_s_sparkle_lsi4d_asin_0_img&pd_rd_w=bhBKh&content-id=amzn1.sym.417820b0-80f2-4084-adb3-fb612550f30b%3Aamzn1.sym.417820b0-80f2-4084-adb3-fb612550f30b&pf_rd_p=417820b0-80f2-4084-adb3-fb612550f30b&pf_rd_r=HKWJGDDWGM9EKW4TGN9E&pd_rd_wg=ARnKN&pd_rd_r=2e33343b-fc30-44ce-aeff-e10bac0a08e0&th=1). The OS booted and we named the first Pi ‚Äú*masterbibble*‚Äù (this entire project is Barbie-themed) and set a password. 

Then we went into the cmdline.txt file and added the following line:
<br>
`cgroup_memory=1 cgroup_enable=memory ip=10.0.0.10::10.0.0.1:255.255.255.0:rpimaster:eth0:off`

We went into the config.txt file and added the following line at the very bottom:
<br>
`arm_64bit=1`

In Powershell, we created an ssh folder in the drive so we could access it using our computer by using the following command:
<br>
`new-item ssh`

To be able to connect to the Pi, we turned off the WiFi and configured the Ethernet port to an IP address (10.0.0.2) within the private network we configured. 

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/436d03de-ace6-4d7a-99e5-b268f0241a32)

<h3>1.2: Raspberry Pi Imaging</h3>

We put the micro SSD back into the Raspberry Pi and connected it to a laptop with an Ethernet cable. Using the command prompt, we SSHed into the Pi using the IP address we configured and it worked! However, we did hit an obstacle that occurred‚Äîwe were unable to use the command iptables (the solution is in section 1.3 iptables). 

**<p align="center">The image below shows *masterbibble* successfully ssh:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/b7960447-188a-4a24-8b7a-652e304bc1b8)

We used the Raspberry Pi Imager v1.8.4 to create an image for the two worker Pis: *workerken* with an IP address of 10.0.0.11 and *workerallen* with an IP address of 10.0.0.12. 

**<p align="center">Image below shows the Raspberry Pi Imager v1.8.4 and the customization settings for workerken and *workerallen*:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/82d8e6f6-ed11-4eb8-b75f-d330e9ca75ad)

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/2f856464-f588-4967-be83-ca2913b71583)

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/0150d5b6-56ff-484a-9a82-c02d298d71c5)

**<p align="center">The images below shows *workerken* and *workerallen* successfully ssh:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/77bc2d96-fa3d-4c7a-ae92-d9f473681953)

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/3872da03-c4a1-4d74-bed8-8c269ff7076b)

<h3>1.3: iptables issue</h3>

After imaging the Raspberry Pis, we found out that iptables was not installed. We were not connected to the WiFi/Internet to be able to download any of the packages because everything had been localized. To solve this problem, we reinstalled the OS on the *masterbibble* Pi to enable wireless LAN connectivity‚Äîthis was also when we changed to the 64-bit OS version.
<br>

However, we doubted that having Ethernet and wireless connectivity active at the same time would allow for a dual connection. There were several ideas like: using the ASA firewall to route our internal network while connected to the school network, setting up NAT, using ad hoc, or simply connecting the Pis to the school‚Äôs guest WiFi to get over the connectivity issue we were facing. Regardless, these solutions were limited by the little authority we had over the school network and we didn‚Äôt want our cluster to be public to the guest network. 

In the end, we decided to use leftover IP addresses that were assigned to the classroom that we were occupying during this project (it was a /24 network). After we had internet access, the Pis updated and installed the needed programs. Once that finished, we configured the Pis be in our local network again. 

To find the network address of the classroom, we connected an ethernet cable to a laptop, disabled the wireless connectivity, and enabled DHCP. We went into the command prompt and used ipconfig to look at the IP address that was assigned. The private network address was 172.30.212.0/24 and so we went back into the network adapter setting to statically change it to 172.30.212.200/24. We chose an IP host address that would least likely be occupied by any device on the network. 

**<p align="center">The image below shows the IP address of the laptop after it was configured to be 172.30.212.200/24:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/ad0e93c5-0219-4d76-9a77-9ddb0efe4ccf)

<h3>1.4: Setting up iptables</h3>
With that knowledge, we went back into all three Pis and statically changed their IP addresses to fit with the network of the classroom. We changed masterbibble to 172.30.212.201/24, workerken to 172.30.212.202/24, and workerallen to 172.30.212.203/24

Finally, we tested whether or not the Pi would have internet access. We used a laptop connected to the same network and ssh into ‚Äò*masterbibble*‚Äô and pinged 1.1.1.1. This successfully pinged!

We continued on and installed iptables, updated, and upgraded it using the following commands for the rest of the Pis:
<br>
1. `sudo apt install iptables`
<br>

2. `sudo apt update`
<br>

3. `sudo apt upgrade`

**<p align="center">Image below shows masterbibble installing iptables:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/4fc91996-1341-4e32-9832-6845096ef75f)

**<p align="center">Image below shows workerken being ssh and successfully ping 1.1.1.1:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/84f09bc4-7a22-4480-9640-2b6de9b6db34)

The next step was to use the following commands and configure the Raspberry Pi. We used the following commands:
<br>

1. `sudo iptables -F`
<br>

2. `sudo update-alternatives --set iptables /usr/sbin/iptables-legacy`
<br>

3. `sudo update-alternatives --set ip6tables /usr/sbin/ip6tables-legacy`
<br>

4. `sudo reboot`
<br>

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/08fe923f-2739-41e7-a3f0-1f890e203cf6)

In the video, we weren‚Äôt sure what commands he used because they were copied and pasted, but luckily we found the set of commands to use in the comment section!
<br>

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/bc7f47cf-d09a-4c5d-9612-626868f1576f)

<h3>1.5: Setting up kubernetes</h3>

To configure the master to use kubernetes, we used the following commands:
<br>

1. `curl -sfL https://gt.k3s.io | K3S_KUBECONFIG_MOD=‚Äù644‚Äù sh -s`
<br>

2. `kubectl get nodes`
<br>

3. `cat /var/lib/rancher/k3s/server/node-token`

**<p align="center">Image below shows masterbibble downloading kubernetes:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/cbc7cc99-292e-405b-933a-99b12f1b37e5)

**<p align="center">Image below shows masterbibble gettings the nodes:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/2f737aa1-5969-4e38-ba15-e397b9d7c868)

**<p align="center">Image below shows masterbibble obtaining the node token:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/65876f8d-a7f5-41a6-8241-ebecc478ebfb)

We went back into the worker Pis and started to install and configure kubernetes onto them (ssh using the NUC ). We input our unique token into the ‚ÄúYOURTOKEN‚Äù spot, changed the [your server] with the master IP address, and changed ‚Äúservername‚Äù with *workerallen*. We used the following command:
<br>
`curl -sfL https://get.k3s.io | K3S_TOKEN="YOURTOKEN" K3S_URL="https://[your server]:6443" K3S_NODE_NAME="servername" sh -`

<p align="center">‚¨áÔ∏è</p>

`curl -sfL https://get.k3s.io | K3S_TOKEN="K10578b5c034ffd32e45f0bd5eff4db97249a4abd718c6df93a50beb39be2b162f6::server:ada46499a2b62c634fd4ee538144e3ff" K3S_URL="https://172.30.212.201:6443" K3S_NODE_NAME="workallen" sh -`

However, there was an error that appeared when inputting the command into the CLI, we got an [ERROR] Only https:// URLs are supported for K3S_URL (have ‚Äúhttps://172.30.212.201:6443‚Äù). 
*Update: It was a semantic error lol. It was the wrong symbol that was copied and pasted. Using a unicode encoder, we can see that although the quotations look similar, they are completely different. The unicodes were not the same :(*

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/5f936e0e-7a6f-4210-89e3-6d4b06bf8ffe)

**<p align="center">Image below shows the ERROR:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/109fa605-51ec-4e53-bcc2-97b2c9a3c001)

**<p align="center">Image below shows the command working correctly:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/467c153f-c434-4109-a0ea-dc89196b6adc)

We proceeded and waited for the agent to load. Going back to the master, we checked if everything worked correctly by using the following command:
<br>
1. `kubectl get nodes`

**<p align="center">Image below shows the successful connection of *workerallen*:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/44441ac5-4181-4dc9-9808-4dd4d2b4a9f5)

The same thing was done on workerken and it was successful. 

**<p align="center">Image below shows the successful connection of workerken:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/73d21409-09a5-4fc3-8454-276bf91cc8f5)

<h3>1.6: Physical Installment</h3>
Finally, it was time to connect everything together and have access to all of the Raspberry Pis at the same time. 
We connected the Raspberry Pi 400 keyboards with the ethernet cables into the switch, powering each respectively. The Pis were placed on top of each other, going from masterbibble down to *workerallen* while trying to maintain somewhat of a good cable management (spoiler alert we don't) by zip tying the ethernet cables. It looks a bit messy, but we worked with the space and materials we had. 

**<p align="center">Image below shows us zip tying the ethernet cable:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/e61baaa0-6d05-4f8a-8711-cbec93f0bf53)

**<p align="center">Image below shows the final result of the cluster:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/7a52c04d-60b8-48a1-a5ce-ec62b60ec412)

<h3>1.7: PuTTy on Windows 11 and Ubuntu 22</h3>
Instead of going to the command line every time we needed to access the cluster, we downloaded PuTTy and created saved sessions to make it easier for us. 

**<p align="center">Image below shows the saved session in PuTTy:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/1db12eb2-34fe-4576-bfd6-7d0a979279ef)

<h3>1.8: Instaling Rancher on Ubuntu 22</h3>
Referencing the NUC section, we installed Ubuntu 22 to work with Rancher. We tried to install Rancher using the following commands:

1. `mkdir /etc/rancher`
<br>

2. `mkdir /etc/rancher/rke2`
<br>

3. `cd /etc/rancher/rke2`
<br>

4. `nano config.yaml`
- a. `token: rancherMon`
- b. `tls-san:`
  - i. `- 172.30.212.200`

5. `curl -sfL https://get.rancher.io | sh`
<br>

6. `systemctl enable rancherd-server.service`
<br>

7. `systemctl start rancherd-server.service`
<br>

8. `journalctl -eu rancherd-server -f`

**<p align="center">Image below shows the text in the nano file:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/91521090-9899-4551-9dd9-c6cbe01b3b53)

Rancher had synced successfully after all of the commands and we reset the admin password using the following command:
<br>
`rancherd reset-admin`

We logged in and added our cluster using the Add Cluster button and clicking on Other Cluster. The cluster name we used was mattel. When we clicked create, there were three different commands to import the cluster, so we chose the last one. We used the following command:
<br>
`curl --insecure -sfL https://172.30.212.200:8443/v3/import/6pprbdwzkgz8l9bl72wlfdl766r9l9ztwr72qmgmfbkx4xq5bttrxq_c-tjkvs.yaml | kubectl apply -f -`

**<p align="center">Image below shows the page that gave the last command:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/21464daa-ab04-4e43-a5c7-c8598721a7f8)

**<p align="center">Image below shows the cluster mattel having a Pending State:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/d42f592b-6a1d-41db-bfc6-a0d4cb3e8de1)

To make the cluster become an Active state, we edited in View API  and input a command in agentImageOverride by using the following command:
<br>
`rancher/rancher-agent:v2.5.17-linux-arm64`

**<p align="center">Image below shows the command used to connect the cluster to Rancher:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/ed02734c-a08c-4033-8a73-02eb789e314a)

*A problem that we encountered was that we couldn't get Rancher to recognize any of the clusters. We were unable to get it to an Active State even after getting an HTTP Response. To try and resolve this issue, we changed the version to what NetworkChuck was using which was v2.5.8 to hopefully get a better outcome. Unfortunately, we were unable to figure out a solution so we moved on to the next part. We hypothesized that the version of our k3s does not support Rancher yet.*

**<p align="center">Image below shows the versions of k3s supporting Rancher:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/d98b2d38-7ce8-4978-80c0-32020c2a8f6f)

After looking into the comment section, we saw one that said we could use a docker container instead. So we decided that after the other configurations, we would go back and check it out later. 

**<p align="center">Image below shows the comment about Rancher:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/e92c6478-08e4-4034-844e-fb5efcfc1194)

<h3>1.9: DEPLOYMENT</h3>
We shifted our focus to trying to deploy an application. The first thing we did was create a YAML file called barbieweb.yaml to insert the code that is referenced below.  

Yaml file: https://gist.github.com/petitviolet/d36f33d145d0bbf4b54eb187b79d0244 

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/64173688-bd7a-4db0-8e73-4b27e2f8267a)

The code is a configuration which uses nginx to start a web server. It's very similar to apache but better. The code that was changed by adding/changing to the file was the following:
<br>
**1. apiVersion: apps/v1**
<br>
**2. ImagePullPolicy: Always**
<br>
**3. httpGET:**

- **a. port: web**
- **b. path**

**4. name: web**
<br>
**5. protocol: TCP**

*We weren't given all the correct commands with the video that was being referenced, so we had to mess around with some of the lines of code, but we got an error for deployment. The problem was hypothesized to be a version problem once again.*

We found a new resource which gave us a new yaml file and tried to give it a go, we made a new file called barbieweb3.yaml.

New yaml file: https://github.com/collabnix/kubelabs/blob/master/pods101/pods01.yaml 

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/02404cb4-4ace-4871-ac7a-18b4b71be505)

Alas, that did not work and so we kept searching for a solution. A new yaml file was created to test called barbieweb4.yaml. Then, we finally found one that worked when we used the following command and yaml code:
<br>
`kubectl apply -f barbieweb4.yaml`

Final yaml file: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ 

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/8d7126bb-9cb7-40d2-bfda-4ef12c270792)

**<p align="center">Image below shows kubectl get pods working successfully:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/9a0a6590-ee9d-4499-88c9-a56169e57ad2)

The next step was to create another yaml file that contained the node port information. The file was called the following:
<br>
*barbienodeport.yaml*

**<p align="center">Image below shows the nano file containing the following lines:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/632491b5-8f50-486c-9c60-e2fca853aeb5)

The code worked successfully and was able to deploy. We went on a web browser and tested whether or not we could access the web browser. 

**<p align="center">Image below shows the command kubectl get services working successfully:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/e6a37669-1647-4430-90b3-a25cc3048f13)

Unfortunately, when we tried accessing the nginx server it was unable to connect when we used the following url:
<br>
*http://172.30.212.201:3111*

To try and troubleshoot the issue, we changed the yaml file and added a few things. Compare the original code to the image provided to see the differences. We assume that the main reason it worked was because we added the httpGet line.

**<p align="center">Image below shows the updated file for barbieweb4.yaml:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/3e2fe2fc-3740-412e-bb1c-1f27954915a2)

Everything seemed to be working and we were finally able to access the nginx server. The other IP addresses that the other clusters used could also successfully connect to the server. 

**<p align="center">Image below shows the nodes running:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/d7eb130e-d770-442d-9a61-583dda742a4d)

**<p align="center">Image below shows the browser being accessible:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/e9fd4a21-5cca-4bed-bd2f-904edc0d0ced)

<h3>1.10: Editing the Server</h3>
<p align="center">TBD</p>

<h3>1.11: Physical Relocation</h3>
To present our network to our Cybersecurity class, we migrated the network to the front of the classroom so the computers in the class could access our cluster. When we connected the Pi Cluster back into the original topology‚Äînothing changed, but we did disconnect the Raspberry Pi Cluster from the firewall because it took down the internet connection from all of the computers üòõ.
<br>
<br>
Our guess to why it took out the internet connection was because it was trying to communicate with one of the firewall interfaces because it was configured to be a default gateway OR there was a duplicate IP address in the arp table. 
<br>
<br>
To show how our network worked, we connected the firewall to show how it protected the Raspberry Pi Cluster against the DoS attack, but also disconnected it to allow other students to DoS it to demonstrate the impact of the firewall. It was very similar to our test runs in the Low Orbit Ion Cannon section. 

**<p align="center">Image below shows how the NUC is connected to the firewall</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/ca930120-36c5-4519-813f-d9f8195e972c)

**<p align="center">Image below shows how the devices are powered on:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/52fecbc3-bdde-4ff3-9fa1-7473010bf6a0)

**<p align="center">Image below shows how the Rapsberry Pi CLuster is connected together:</p>**

![image](https://github.com/itsvivianmill/Raspberry-Pi-Cluster/assets/116047994/826fb7ec-d87e-413f-9b8c-ee6b44ab8403)
